# Voxceleb2 pretraining configuration
# This file contains all the parameters for pretraining the facial foundation model

model:
  name: pretrain_videomae_base_dim512_no_depth_patch16_160
  tubelet_size: 2
  input_size: 160
  drop: 0.0
  attn_drop_rate: 0.0
  drop_path: 0.1
  use_mean_pooling: True
  mask_ratio: 0.9
  encoder_depth: 16
  decoder_depth: 4
  normalize_target: true
  # depth: 16
  
  # Attention configuration
  attn_type: local_global
  part_win_size: [2, 5, 10]
  lg_region_size: [2, 5, 10]
  lg_first_attn_type: self
  lg_third_attn_type: cross
  lg_attn_param_sharing_first_third: false
  lg_attn_param_sharing_all: false
  lg_classify_token_type: org
  lg_no_second: false
  lg_no_third: false

data:
  dataset_name: voxceleb2
  data_path: saved/data/voxceleb2/info_clean.csv
  eval_data_path: null
  num_classes: 2
  num_segments: 1
  num_frames: 16
  sampling_rate: 4
  imagenet_default_mean_and_std: true
  
  # Data loading
  batch_size: 4  # Small batch size for pretraining
  num_workers: 4
  pin_memory: true

augmentation:
  # Basic augmentation
  color_jitter: 0.4
  train_interpolation: bicubic
  num_sample: 1  # Disabled for gaze estimation
  
  # Auto augmentation (disabled for gaze)
  # auto_augment: null
  
  # Mixup augmentation (disabled for gaze estimation)
  mixup: 0.0
  cutmix: 0.0
  cutmix_minmax: null
  mixup_prob: 1.0
  mixup_switch_prob: 0.5
  mixup_mode: batch
  
  # Random erase augmentation
  random_erase_prob: 0.25
  random_erase_mode: pixel
  random_erase_count: 1
  random_erase_split: false
  
  # Label smoothing (disabled for regression)
  label_smoothing: 0.0

optimization:
  optimizer: adamw
  lr: 0.0003
  min_lr: 0.000001
  warmup_lr: 0.000001
  weight_decay: 0.05
  weight_decay_end: null
  
  # Learning rate schedule
  warmup_epochs: 5
  warmup_steps: -1
  
  # Optimizer specific
  opt_eps: 0.00000001
  opt_betas: [0.9, 0.95]
  momentum: 0.9
  
  # Gradient clipping
  clip_grad: null
  layer_decay: 0.75

training:
  epochs: 100
  start_epoch: 0
  update_freq: 1
  save_ckpt_freq: 10
  
  # Model EMA
  model_ema: false
  model_ema_decay: 0.9999
  model_ema_force_cpu: false
  
  # Evaluation (disabled for pretraining)
  disable_eval_during_finetuning: true
  eval_only: false
  dist_eval: false
  
  # Checkpointing
  save_ckpt: true
  auto_resume: true
  resume: ''
  
  # Validation metric (not used in pretraining)
  val_metric: loss

pretraining:
  # Frame difference as target
  use_frame_diff_as_target: true
  frame_diff_group_size: 2
  target_diff_weight: null

system:
  device: cuda
  seed: 0
  output_dir: ./output/voxceleb2_pretrain_yaml_config
  log_dir: ./output/voxceleb2_pretrain_yaml_config
  
  # Distributed training
  world_size: 1  # 设置为您的GPU数量
  local_rank: -1
  dist_on_itp: false
  dist_url: env://
  distributed: false  # 根据需要启用分布式训练
  dist_backend: nccl
  
  # DeepSpeed (advanced optimization)
  enable_deepspeed: false
  
  # Features
  save_feature: false
